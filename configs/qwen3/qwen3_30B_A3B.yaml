# Qwen3 30B A3B (Mixture of Experts) Model Configuration
# Model Architecture
vocab_size: 151936                    # Vocabulary size
context_length: 262144                # Maximum sequence length (extended)
embedding_dim: 2048                   # Token embedding dimension
num_heads: 32                         # Number of attention heads
num_layers: 48                        # Number of transformer blocks
hidden_dim: 768                       # Base feed-forward hidden dimension
head_dim: 128                         # Dimension per attention head
num_kv_groups: 4                      # Number of key-value groups for GQA
d_input: 2048                         # Input dimension (same as embedding_dim)

# Attention Configuration
use_qk_norm: true                     # Use RMSNorm on queries and keys
q_proj_use_bias: false               # Query projection bias
k_proj_use_bias: false               # Key projection bias
v_proj_use_bias: false               # Value projection bias
o_proj_use_bias: false               # Output projection bias

# Feed Forward Configuration
fc_use_bias: false                    # Feed-forward layer bias

# Layer Normalization Configuration
epsilon: 1.0e-6                       # Small value for numerical stability
qwen3_compatiable: true               # Qwen3 compatibility mode
rms_norm_use_bias: false              # RMSNorm bias parameter

# Mixture of Experts Configuration
num_experts: 128                      # Total number of experts
num_experts_per_tok: 8                # Number of experts to route each token
moe_intermediate_size: 768            # Hidden dimension for MoE layers

# Positional Encoding
rope_base: 10000000.0                 # RoPE base frequency (extended for long context)

# Training Configuration
dtype: bfloat16                       # Model parameter data type