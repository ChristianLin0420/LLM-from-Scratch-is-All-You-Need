# Attention Mechanisms

## Overview

This document compares the different attention mechanisms implemented in the LLM from Scratch framework.

## Multi-Head Attention (MHA) vs Grouped Query Attention (GQA)

### Multi-Head Attention (Qwen1)

```
ğŸ“Š Standard Multi-Head Attention Flow
â”‚
Input: [batch_size, seq_len, d_model]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear Projections (Equal Size) â”‚
â”œâ”€ Q: d_model â†’ num_heads Ã— head_dim
â”œâ”€ K: d_model â†’ num_heads Ã— head_dim
â””â”€ V: d_model â†’ num_heads Ã— head_dim
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Positional Encoding             â”‚
â”œâ”€ Apply RoPE to Q
â””â”€ Apply RoPE to K
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attention Computation           â”‚
â”œâ”€ Q @ K^T / âˆšhead_dim
â”œâ”€ Apply causal mask
â”œâ”€ Softmax
â””â”€ Apply to V
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output Projection
    â†“
Output: [batch_size, seq_len, d_model]
```

**Characteristics:**
- Equal number of heads for Q, K, V
- Full KV cache requirements
- Standard transformer attention

### Grouped Query Attention (Qwen3)

```
ğŸ“Š Grouped Query Attention Flow
â”‚
Input: [batch_size, seq_len, d_model]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear Projections (Grouped)    â”‚
â”œâ”€ Q: d_model â†’ num_heads Ã— head_dim
â”œâ”€ K: d_model â†’ num_kv_groups Ã— head_dim  â­ Fewer heads
â””â”€ V: d_model â†’ num_kv_groups Ã— head_dim  â­ Fewer heads
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QK Normalization + RoPE         â”‚
â”œâ”€ Apply RMSNorm to Q â­ Unique
â”œâ”€ Apply RMSNorm to K â­ Unique
â”œâ”€ Apply RoPE to Q
â””â”€ Apply RoPE to K
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Group Expansion                 â”‚
â”œâ”€ Expand K: kv_groups â†’ num_heads
â””â”€ Expand V: kv_groups â†’ num_heads
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attention Computation           â”‚
â”œâ”€ Q @ K^T / âˆšhead_dim
â”œâ”€ Apply causal mask
â”œâ”€ Softmax
â””â”€ Apply to V
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Output Projection
    â†“
Output: [batch_size, seq_len, d_model]
```

**Characteristics:**
- Fewer KV heads than Q heads
- Reduced memory usage
- Shared KV heads across query groups

## Memory Comparison

| Model | Q Heads | K Heads | V Heads | KV Cache Reduction |
|-------|---------|---------|---------|-------------------|
| **Qwen1-7B** | 32 | 32 | 32 | 1Ã— (baseline) |
| **Qwen3-8B** | 32 | 16 | 16 | 2Ã— reduction |
| **Qwen3-32B** | 64 | 32 | 32 | 2Ã— reduction |

## Implementation Details

### Standard MHA Implementation

```python
def multi_head_attention(q, k, v, mask):
    # q, k, v: [batch, num_heads, seq_len, head_dim]
    scores = torch.matmul(q, k.transpose(-2, -1))
    scores = scores / math.sqrt(head_dim)
    scores = scores.masked_fill(mask, float('-inf'))
    attn_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, v)
    return output
```

### Grouped Query Attention Implementation

```python
def grouped_query_attention(q, k, v, mask, num_kv_groups):
    # q: [batch, num_heads, seq_len, head_dim]
    # k, v: [batch, num_kv_groups, seq_len, head_dim]
    
    group_size = num_heads // num_kv_groups
    
    # Expand k, v to match q
    k = k.repeat_interleave(group_size, dim=1)
    v = v.repeat_interleave(group_size, dim=1)
    
    # Standard attention computation
    scores = torch.matmul(q, k.transpose(-2, -1))
    scores = scores / math.sqrt(head_dim)
    scores = scores.masked_fill(mask, float('-inf'))
    attn_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attn_weights, v)
    return output
```

## Performance Analysis

### Memory Usage

```
ğŸ“Š Memory Usage Comparison

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MHA Memory Usage (Qwen1)     â”‚  â”‚    GQA Memory Usage (Qwen3)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                 â”‚  â”‚                                 â”‚
â”‚ Q Cache: num_heads Ã— seq Ã— head â”‚  â”‚ Q Cache: num_heads Ã— seq Ã— head â”‚
â”‚ K Cache: num_heads Ã— seq Ã— head â”‚  â”‚ K Cache: kv_groups Ã— seq Ã— head â”‚
â”‚ V Cache: num_heads Ã— seq Ã— head â”‚  â”‚ V Cache: kv_groups Ã— seq Ã— head â”‚
â”‚                                 â”‚  â”‚                                 â”‚
â”‚ Total KV: 2 Ã— num_heads Ã— ...   â”‚  â”‚ Total KV: 2 Ã— kv_groups Ã— ...   â”‚
â”‚                                 â”‚  â”‚                                 â”‚
â”‚ Example (32 heads):             â”‚  â”‚ Example (32 Q, 16 KV):          â”‚
â”‚ â”œâ”€ Q: 32 Ã— seq Ã— 128            â”‚  â”‚ â”œâ”€ Q: 32 Ã— seq Ã— 128            â”‚
â”‚ â”œâ”€ K: 32 Ã— seq Ã— 128            â”‚  â”‚ â”œâ”€ K: 16 Ã— seq Ã— 128 â­         â”‚
â”‚ â””â”€ V: 32 Ã— seq Ã— 128            â”‚  â”‚ â””â”€ V: 16 Ã— seq Ã— 128 â­         â”‚
â”‚                                 â”‚  â”‚                                 â”‚
â”‚ KV Memory: 100% (baseline)      â”‚  â”‚ KV Memory: 50% (2Ã— reduction)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Speed Comparison

| Operation | MHA | GQA | Speedup |
|-----------|-----|-----|---------|
| **Forward Pass** | 1.0Ã— | 1.1Ã—-1.3Ã— | Modest |
| **KV Cache** | 1.0Ã— | 2.0Ã—-4.0Ã— | Significant |
| **Memory Bandwidth** | 1.0Ã— | 1.5Ã—-2.5Ã— | Good |

## QK Normalization

Only available in Qwen3 (GQA):

```python
# Apply normalization to queries and keys
if self.q_norm:
    q = self.q_norm(q)  # RMSNorm on queries
if self.k_norm:
    k = self.k_norm(k)  # RMSNorm on keys
```

**Benefits:**
- Improved training stability
- Better gradient flow
- Enhanced long-context performance