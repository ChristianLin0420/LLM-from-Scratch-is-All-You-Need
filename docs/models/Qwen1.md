# Qwen1 Model Documentation

## ğŸ¯ Overview

Qwen1 implements the classic transformer architecture with standard Multi-Head Attention (MHA). This model provides a simpler, more traditional approach compared to Qwen3, making it ideal for research, experimentation, and understanding fundamental transformer concepts.

## ğŸ—ï¸ Architecture

### High-Level Architecture

```
ğŸ—ï¸ Qwen1 Model Architecture
â”‚
Input IDs
    â†“
Token Embedding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qwen1 Blocks (x N layers)       â”‚
â”‚                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Single Qwen1 Block          â”‚ â”‚
â”‚ â”‚                             â”‚ â”‚
â”‚ â”‚ Input                       â”‚ â”‚
â”‚ â”‚   â†“                         â”‚ â”‚
â”‚ â”‚ RMSNorm                     â”‚ â”‚
â”‚ â”‚   â†“                         â”‚ â”‚
â”‚ â”‚ Multi-Head Attention        â”‚ â”‚
â”‚ â”‚   â†“                         â”‚ â”‚
â”‚ â”‚ Residual Connection         â”‚ â”‚
â”‚ â”‚   â†“                         â”‚ â”‚
â”‚ â”‚ RMSNorm                     â”‚ â”‚
â”‚ â”‚   â†“                         â”‚ â”‚
â”‚ â”‚ Feed Forward Network        â”‚ â”‚
â”‚ â”‚   â†“                         â”‚ â”‚
â”‚ â”‚ Residual Connection         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final RMSNorm
    â†“
Language Model Head
    â†“
Output Logits

ğŸ“‹ Multi-Head Attention Details:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input â†’ Q, K, V Projections     â”‚
â”‚           â†“    â†“    â†“           â”‚
â”‚         RoPE RoPE   V           â”‚
â”‚           â†“    â†“    â†“           â”‚
â”‚      Standard Attention         â”‚
â”‚           â†“                     â”‚
â”‚      Output Projection          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

#### 1. Standard Multi-Head Attention (MHA)
```
ğŸ“Š Multi-Head Attention Flow
â”œâ”€ Input: [batch_size, seq_len, d_model]
â”‚
â”œâ”€ Linear Projections:
â”‚  â”œâ”€ Q Projection â†’ [batch_size, seq_len, num_heads, head_dim]
â”‚  â”œâ”€ K Projection â†’ [batch_size, seq_len, num_heads, head_dim]
â”‚  â””â”€ V Projection â†’ [batch_size, seq_len, num_heads, head_dim]
â”‚
â”œâ”€ Positional Encoding:
â”‚  â”œâ”€ Apply RoPE to Q
â”‚  â””â”€ Apply RoPE to K
â”‚
â”œâ”€ Attention Computation:
â”‚  â”œâ”€ Transpose to [batch_size, num_heads, seq_len, head_dim]
â”‚  â”œâ”€ Compute attention scores: Q @ K^T / âˆšhead_dim
â”‚  â”œâ”€ Apply causal mask
â”‚  â”œâ”€ Softmax normalization
â”‚  â””â”€ Apply to values: attention_weights @ V
â”‚
â”œâ”€ Reshape & Project:
â”‚  â”œâ”€ Concatenate heads â†’ [batch_size, seq_len, d_model]
â”‚  â””â”€ Output projection
â”‚
â””â”€ Output: [batch_size, seq_len, d_model]
```

**MHA Characteristics:**
- **Simplicity**: Equal number of heads for Q, K, V
- **Standard Implementation**: Classic transformer attention
- **Research Friendly**: Well-understood and documented

#### 2. Feed Forward Network (SwiGLU)
```
ğŸ”€ SwiGLU Feed Forward Network
â”‚
Input
â”œâ”€ Linear 1 (Gate) â”€â”€â”
â”‚                    â”œâ”€ Element-wise Multiply â”€â”
â”œâ”€ Linear 2 (Up) â”€â”€â”€â”€â”¤                         â”‚
   â”‚                 â”‚                         â”‚
   â””â”€ SiLU Activationâ”€â”˜                        â”‚
                                               â”‚
                                               â†“
                                         Linear 3 (Down)
                                               â”‚
                                               â†“
                                            Output

ğŸ” Component Details:
â”œâ”€ Gate: d_model â†’ hidden_dim
â”œâ”€ Up:   d_model â†’ hidden_dim  
â”œâ”€ SiLU: Sigmoid Linear Unit activation
â”œâ”€ Multiply: Gate(x) * SiLU(Up(x))
â””â”€ Down: hidden_dim â†’ d_model
```

#### 3. Positional Encoding (Standard RoPE)
```
ğŸ”„ Rotary Position Embedding (RoPE)
â”‚
Query/Key Tensors [batch, seq_len, heads, head_dim]
    â†“
Split head_dim into pairs: [(d0,d1), (d2,d3), ...]
    â†“
Position Calculation:
â”œâ”€ Position indices: [0, 1, 2, ..., seq_len-1]
â”œâ”€ Frequency calculation: 10000^(-2i/head_dim)
â”œâ”€ Angles: position Ã— frequency
â””â”€ Sin/Cos values: sin(angles), cos(angles)
    â†“
Apply Rotation:
â”œâ”€ For each pair (x, y):
â”‚  â”œâ”€ x_new = x * cos - y * sin
â”‚  â””â”€ y_new = x * sin + y * cos
â””â”€ Concatenate rotated pairs
    â†“
Rotated Query/Key Tensors

ğŸ¯ RoPE Benefits:
â”œâ”€ Relative position encoding
â”œâ”€ Extrapolation to longer sequences
â””â”€ Preserves attention patterns
```

## ğŸ“Š Model Specifications

### Available Configurations

| Model | Parameters | Embedding Dim | Layers | Heads | Head Dim | Context Length |
|-------|------------|---------------|--------|-------|----------|----------------|
| **Qwen1-1.8B** | 1.8B | 2048 | 24 | 16 | 128 | 8K |
| **Qwen1-7B** | 7B | 4096 | 32 | 32 | 128 | 8K |
| **Qwen1-14B** | 14B | 5120 | 40 | 40 | 128 | 8K |
| **Qwen1-32B** | 32B | 5120 | 64 | 40 | 128 | 8K |

### Configuration Parameters

```yaml
# Example: Qwen1 7B Configuration
vocab_size: 151936
context_length: 8192          # Shorter than Qwen3
embedding_dim: 4096
num_heads: 32                 # Same for Q, K, V (no grouping)
num_layers: 32
hidden_dim: 11008
head_dim: 128

# Simplified features
use_qk_norm: false            # No QK normalization
rope_base: 10000.0            # Standard RoPE frequency
qwen3_compatiable: false      # Different architecture
```

## ğŸš€ Usage Examples

### Basic Usage

```python
from modules.load_model import load_qwen1

# Load Qwen1 model
model, tokenizer, config = load_qwen1(
    size="7B",
    variant="chat",
    device="cuda"
)

# Generate text
from modules.text_generation import TextGenerator
generator = TextGenerator(model, tokenizer, device="cuda")

response = generator.chat("Explain standard multi-head attention")
print(response)
```

### Research & Experimentation

```python
from modules.load_model import load_pretrained_model

# Load smaller model for quick experiments
model, tokenizer, config = load_pretrained_model(
    model_type="qwen1",
    size="1.8B",
    variant="chat",
    device="cuda"
)

# Test different attention patterns
def analyze_attention_patterns(model, text):
    # Your research code here
    pass
```

### Command Line Usage

```bash
# Available sizes: 1.8B, 7B, 14B, 32B
python main.py --model qwen1 --size 1.8B --prompt "Explain transformers"

# Interactive mode for experiments
python main.py --model qwen1 --size 7B --interactive

# Compare with Qwen3
python main.py --model qwen1 --size 14B --benchmark
python main.py --model qwen3 --size 14B --benchmark
```

## ğŸ”§ Technical Details

### Multi-Head Attention Implementation

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        self.num_heads = config["num_heads"]
        self.head_dim = config["head_dim"]
        self.d_output = self.num_heads * self.head_dim
        
        # Standard MHA: same number of heads for Q, K, V
        self.q_proj = nn.Linear(d_input, self.d_output)
        self.k_proj = nn.Linear(d_input, self.d_output)  # Same size as Q
        self.v_proj = nn.Linear(d_input, self.d_output)  # Same size as Q
        
        self.output_proj = nn.Linear(self.d_output, d_input)
    
    def forward(self, x, attention_mask, cos, sin):
        # Project to Q, K, V
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        
        # Reshape for multi-head attention
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # Apply RoPE
        q = apply_rope(q, cos, sin)
        k = apply_rope(k, cos, sin)
        
        # Transpose for attention computation
        q = q.transpose(1, 2)  # [batch, heads, seq, head_dim]
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # Standard attention computation
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        scores = scores.masked_fill(attention_mask, float('-inf'))
        attn_weights = F.softmax(scores, dim=-1)
        
        context = torch.matmul(attn_weights, v)
        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, seq_len, self.d_output)
        
        return self.output_proj(context)
```

### Weight Mapping

Qwen1 weights are mapped from HuggingFace format (similar to Qwen3 but without QK norm):

```python
# Attention weights (same structure as Qwen3)
model.blocks[i].attention.q_proj.weight â† params[f"model.layers.{i}.self_attn.q_proj.weight"]
model.blocks[i].attention.k_proj.weight â† params[f"model.layers.{i}.self_attn.k_proj.weight"]
model.blocks[i].attention.v_proj.weight â† params[f"model.layers.{i}.self_attn.v_proj.weight"]
model.blocks[i].attention.output_proj.weight â† params[f"model.layers.{i}.self_attn.o_proj.weight"]

# No QK normalization weights (key difference from Qwen3)

# Feed Forward weights (same as Qwen3)
model.blocks[i].ffn.fc1.weight â† params[f"model.layers.{i}.mlp.gate_proj.weight"]
model.blocks[i].ffn.fc2.weight â† params[f"model.layers.{i}.mlp.up_proj.weight"]
model.blocks[i].ffn.fc3.weight â† params[f"model.layers.{i}.mlp.down_proj.weight"]
```

## ğŸ“ˆ Performance Characteristics

### Memory Usage
- **Standard MHA**: Full KV cache (no grouping)
- **Predictable Memory**: Linear scaling with sequence length
- **Research Friendly**: Easier to analyze and modify

### Speed Benchmarks
| Model Size | Device | Throughput (tok/s) | Memory (GB) | Context Window |
|------------|--------|-------------------|-------------|----------------|
| 1.8B | GPU | 100-200 | 4-8 | 8K |
| 7B | GPU | 30-60 | 16-24 | 8K |
| 14B | GPU | 15-30 | 32-48 | 8K |
| 32B | GPU | 5-15 | 80-120 | 8K |

## ğŸ” Architecture Comparison: Qwen1 vs Qwen3

### Key Differences

| Feature | Qwen1 | Qwen3 | Impact |
|---------|-------|--------|--------|
| **Attention** | Multi-Head Attention | Grouped Query Attention | Memory usage, Speed |
| **QK Normalization** | âŒ No | âœ… Yes | Training stability |
| **Context Length** | 8K tokens | 32K-262K tokens | Long document handling |
| **RoPE Base** | 10,000 | 1,000,000 | Position encoding range |
| **Complexity** | Simple | Advanced | Implementation difficulty |

### When to Choose Qwen1

**Research & Education:**
- Understanding transformer fundamentals
- Implementing custom attention mechanisms
- Debugging and analysis
- Quick prototyping

**Resource Constraints:**
- Limited memory environments
- Simpler deployment requirements
- CPU-only inference

**Specific Use Cases:**
- Short-context applications
- Traditional NLP tasks
- Model comparison studies

### Architecture Visualization: Side-by-Side

```
ğŸ“Š Qwen1 vs Qwen3 Attention Comparison

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Qwen1 (Standard MHA)     â”‚  â”‚   Qwen3 (Grouped QA)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input                       â”‚  â”‚ Input                       â”‚
â”‚   â†“                         â”‚  â”‚   â†“                         â”‚
â”‚ Q/K/V Projections           â”‚  â”‚ Q/K/V Projections           â”‚
â”‚ â”œâ”€ Q: num_heads             â”‚  â”‚ â”œâ”€ Q: num_heads             â”‚
â”‚ â”œâ”€ K: num_heads             â”‚  â”‚ â”œâ”€ K: num_kv_groups         â”‚
â”‚ â””â”€ V: num_heads             â”‚  â”‚ â””â”€ V: num_kv_groups         â”‚
â”‚   â†“                         â”‚  â”‚   â†“                         â”‚
â”‚ RoPE (Q, K)                 â”‚  â”‚ QK Normalization + RoPE     â”‚
â”‚   â†“                         â”‚  â”‚   â†“                         â”‚
â”‚ Standard Attention          â”‚  â”‚ Grouped Attention           â”‚
â”‚ (All heads independent)     â”‚  â”‚ (Shared K/V across groups)  â”‚
â”‚   â†“                         â”‚  â”‚   â†“                         â”‚
â”‚ Output                      â”‚  â”‚ Output                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ” Key Differences:
â”œâ”€ Qwen1: Equal heads for Q, K, V
â”œâ”€ Qwen3: Fewer K/V heads (grouped)
â”œâ”€ Qwen1: No QK normalization
â”œâ”€ Qwen3: QK normalization for stability
â”œâ”€ Qwen1: Higher memory usage
â””â”€ Qwen3: Reduced memory, faster inference
```

## ğŸ› ï¸ Development Notes

### Repository Patterns
```python
# HuggingFace repository patterns
"base": "Qwen/Qwen-{size}-Chat"
"instruct": "Qwen/Qwen-{size}-Chat"
"chat": "Qwen/Qwen-{size}-Chat"
```

### Configuration Files
- `configs/qwen1/qwen1_1.8B.yaml`
- `configs/qwen1/qwen1_7B.yaml`
- `configs/qwen1/qwen1_14B.yaml`
- `configs/qwen1/qwen1_32B.yaml`

### Implementation Files
- `modules/llm/qwen1.py` - Main model class
- `modules/block/qwen1_block.py` - Transformer block
- `modules/attention/multi_head_attention.py` - MHA implementation

## ğŸ”§ Customization Examples

### Research Modifications

```python
# Example: Custom attention analysis
class AnalyzableQwen1Block(Qwen1Block):
    def forward(self, x, attention_mask, cos, sin):
        # Store attention weights for analysis
        hidden_states = self.norm1(x)
        attention_output, attention_weights = self.attention(
            hidden_states, attention_mask, cos, sin, return_attention=True
        )
        
        # Log attention patterns
        self.log_attention_patterns(attention_weights)
        
        # Continue normal forward pass
        x = x + attention_output
        residual = x
        x = self.norm2(x)
        ffn_output = self.ffn(x)
        x = residual + ffn_output
        
        return x
```

### Configuration Customization

```yaml
# Custom Qwen1 configuration for experiments
vocab_size: 151936
context_length: 4096          # Shorter for faster experiments
embedding_dim: 2048           # Smaller for resource constraints
num_heads: 16                 # Reduced complexity
num_layers: 20                # Fewer layers
hidden_dim: 5504

# Experimental features
use_qk_norm: false
rope_base: 10000.0
dtype: float32                # Full precision for research
```

## ğŸ“š Related Documentation

- [Qwen3 Model](Qwen3.md) - Advanced architecture comparison
- [Attention Mechanisms](../architecture/attention.md) - MHA vs GQA deep dive
- [Model Development](../development/adding-models.md) - How Qwen1 was implemented
- [Performance Comparison](../architecture/performance.md) - Qwen1 vs Qwen3 benchmarks

---

*For implementation details, see the source code in `modules/llm/qwen1.py` and `modules/block/qwen1_block.py`*